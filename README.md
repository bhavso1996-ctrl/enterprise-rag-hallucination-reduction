# Enterprise RAG Hallucination Reduction

This project focuses on building a question-answering system that works on enterprise documents like policies and manuals. The goal is to reduce hallucinations by making sure the AI only answers using information that actually exists in the documents. If the answer cannot be found, the system should say it does not know instead of guessing. This project is part of learning and experimenting with reliable LLM and retrieval-augmented generation (RAG) systems.
